{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Language Models, Generative AI &amp; Sequence Modeling</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 09:35:44.180581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.saving import load_model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "     <li>The code is based closely on code from: \n",
    "        A. G&eacute;ron: \n",
    "        <i>Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow (3rd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Warning</h1>\n",
    "<ul>\n",
    "    <li>The code takes a very long time to run.\n",
    "    </li>\n",
    "    <li>It is not important to understand this code in any case.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Models</h1>\n",
    "<ul>\n",
    "    <li>A <b>language model</b> for a given natural language, such as English, estimates the probability of\n",
    "        each possible string of words, e.g.\n",
    "        <ul>\n",
    "            <li>P(\"The dog chased the cat\") = 0.000002</li>\n",
    "            <li>P(\"The cat chased the dog\") = 0.0000002</li>\n",
    "            <li>P(\"The the chased cat dog\") = 0.000000000000001</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If we have a <b>character-level language model</b>, then we can predict the most-likely next character.\n",
    "        <ul>\n",
    "            <li>E.g. P(\"h\" | \"The dog chased t\") = 0.9, P(\"w\" | \"The dog chased t\") = 0.05, P(\"x\" | \"The dog chased t\") = 0.00001</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If we have a <b>word-level language model</b>, then we can predict the most-likely next word.\n",
    "        <ul>\n",
    "            <li>E.g. P(\"the\" | \"The dog chased\") = 0.07, P(\"a\" | \"The dog chased\") = 0.0689, P(\"walked\" | \"The dog chased\") = 0.0000004</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning a language model</h2>\n",
    "<ul>\n",
    "    <li>If we have lots of text, we can learn a language model.</li>\n",
    "    <li>A simple-minded approach (using a word-level language model by way of example):\n",
    "        <ul>\n",
    "            <li>For each word, count next-word frequencies in the training examples.</li>\n",
    "            <li>E.g. in the training examples, \"the\" is followed by \"dog\" 20 times, by \"cat\" 15 times, \"kangaroo\" once, and so on.\n",
    "            </li>\n",
    "            <li>From these, we can calculate the probabilities.</li>\n",
    "        </ul>\n",
    "        What is the weakness of this?\n",
    "    </li>\n",
    "    <li>So, instead, AI researchers use neural networks.</li>\n",
    "    <li>We'll illustrate with a character-level language model.\n",
    "        <ul>\n",
    "            <li>An advantage of character-level models is we have a small number of next possible characters.</li>\n",
    "            <li>For word-level models, on the other hand, we have to decide on a vocabulary and how to handle words that fall outside the vocabulary.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A Character-Level Language Model using a RNN</h2>\n",
    "<ul>\n",
    "    <li>Everyone does this on Shakespeare &mdash; perhaps because if it outputs bad\n",
    "        Shakespeare some people still think it sounds like Shakespeare!\n",
    "    </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing the training data</h3>\n",
    "<ul>\n",
    "    <li>Most of the effort goes into preprocessing the dataset. Don't get bogged down in the details of this code.</li>\n",
    "    <li>We're one-hot encoding the characters.</li>\n",
    "    <li>We're making overlapping windows, shuffling these, and putting them into batches.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much Shakespeare are we working with? How many characters?\n",
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you the first part of it\n",
    "shakespeare_text[:148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you all its distinct characters\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a character-level (rather than word-level) tokenizer\n",
    "# In effect, it lowercases and assigns ids to characters from 1 to 39 inc, e.g. ' ' is 2, 'e' is 3, etc.\n",
    "# However, UNK is 0 and padding is 1. We don't want these so we subtract 2. Now, e.g. ' ' is 0, 'e' is 1, etc.\n",
    "vectorization_layer = TextVectorization(split=\"character\", standardize=\"lower\")\n",
    "vectorization_layer.adapt([shakespeare_text])\n",
    "encoded = vectorization_layer([shakespeare_text])[0]\n",
    "encoded -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 9, 24,  3,  6, 26])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you an encoding\n",
    "vectorization_layer(\"speak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = vectorization_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chararacters = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us create training, validation and test sets.\n",
    "\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    # window() splits the data into smaller windows of text\n",
    "    # Using shift=1 means the first window is, e.g., characters 0 to 100, the second is characters 1 to 101, etc.\n",
    "    # Using drop_remainder=True means all windows are 101 characters long without needing us to pad the last ones (they are dropped)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    # But window() produces a nested dataset: a dataset containing windows (each of which is a dataset) so we flatten it\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    # Shuffle the windows and put into batches\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    # Separate the inputs (the first 100 characters) from the targets (the last, i.e. 101st, character)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "\n",
    "train_set = to_dataset(encoded[:1_000_000], length=max_length, shuffle=True)\n",
    "val_set = to_dataset(encoded[1_000_000:1_060_000], length=max_length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The RNN</h3>\n",
    "<ul>\n",
    "    <li>We'll embed the tokenized input.</li>\n",
    "    <li>Then we'll use a single GRU layer.</li>\n",
    "    <li>The output layer has <code>max_tokens</code> neurons, because we're predicting that number of\n",
    "        distinct characters, i.e. we have <code>max_tokens</code> classes.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,))\n",
    "x = Embedding(input_dim=max_tokens, output_dim=16)(inputs)\n",
    "x = GRU(128, activation=\"tanh\", return_sequences=True)(x)\n",
    "outputs = Dense(max_tokens, activation=\"softmax\")(x)\n",
    "char_language_model = Sequential(Model(inputs, outputs))\n",
    "\n",
    "char_language_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either run this, which takes a very, very long time\n",
    "history = char_language_model.fit(train_set, validation_data=val_set, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or simply run this to load one that I saved for you previously\n",
    "char_language_model = load_model(\"models/char_lm.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Self-supervised learning</h3>\n",
    "<ul>\n",
    "    <li>Hold on! We are doing supervised learning. But our dataset has no labels. It is simply a lot of\n",
    "        text.\n",
    "    </li>\n",
    "    <li>So, what are we using as labels?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predictions using the language model</h3>\n",
    "<ul>\n",
    "    <li>Given some text (the <b>prompt</b>), we can use the model to predict the next character.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to preprocess the text whose next character we will predict (tokenize and subtract 2) and make a prediction\n",
    "def predict_next_char(text):\n",
    "    x = np.array(vectorization_layer([text])) - 2\n",
    "    probabilities = char_language_model.predict(x, verbose=0)[0, -1]\n",
    "    predicted_token = tf.argmax(probabilities)\n",
    "    predicted_char = vectorization_layer.get_vocabulary()[predicted_token + 2]\n",
    "    return predicted_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char(\"To be or not to b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char([\"How are yo\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word-Level Language Models</h2>\n",
    "<ul>\n",
    "    <li>The ideas are similar but the network predicts words instead of characters.</li>\n",
    "    <li>However, there are a couple of subtelties.</li>\n",
    "    <li>They are presented briefly below, but do not worry about them (e.g. do not `learn off').</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sampled softmax</h3>\n",
    "<ul>\n",
    "    <li>The output layer of the character-level language model has one neuron per possible <em>character</em>; \n",
    "        see <code>Dense(max_id,...)</code> above. E.g. if there are 39 possible characters, then there\n",
    "        are 39 neurons in this layer. It outputs 39 probabilities.\n",
    "    </li>\n",
    "    <li>The output layer of a word-level language model has one neuron per <em>word</em> in our vocabulary: tens- or \n",
    "        hundreds-of-thousands of neurons; tens- or hundreds-of-thousands probabilities. The softmax activation function would have to sum\n",
    "        over the outputs of all the neurons. This is OK if there a few dozen (character-level language\n",
    "        model) but not if there are thousands (word-level model).\n",
    "    </li>\n",
    "    <li>One solution that helps speed-up training is called sampled softmax.\n",
    "        Without going into the details, in sampled softmax, the loss is estimated from a <em>sample</em>\n",
    "        of the outputs, instead of all of them.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformer decoders for word-level language models</h3>\n",
    "<ul>\n",
    "    <li>It's fair to say that most word-level language models these days are <b>not</b> built using RNNs.</li>\n",
    "    <li>Instead, we use transformers.</li>\n",
    "    <li>As we know, an RNN takes in one input in each time step (e.g. it takes in one word at a time). Transformers, on the other\n",
    "        hand, receive the entire input sequence in one go. (It arrives as a matrix with one word encoding per row, and this is\n",
    "        why we needed positional encoding.) This allows what is sometimes referred to as <b>bidrectional</b> understanding: its\n",
    "        understanding of a word depends on words that come before it, and words that come after it. (By the way, the word bidirectional \n",
    "        seems unfortunate to me, because the word already had a similar but slightly different meaning for RNNs.)</li>\n",
    "    <li>However, to build a language model similar to the one above, bidirectional understanding is not what we want.</li>\n",
    "    <li>This leads us to distinguish transformer encoders and decoders.\n",
    "        <ul>\n",
    "            <li>For text classificatiion tasks (such as the movie review sentiment analysis we were doing in the previous few lectures),\n",
    "                we want a transformer encoder, with bidirectional understanding.\n",
    "            </li>\n",
    "            <li>For autoregressive tasks (predicting the next item from the previous ones), it would be cheating to work bidirectionally. \n",
    "                A transformer decoder is limited to making its predictions based on only the previous items. In a word-level \n",
    "                language model that\n",
    "                uses a transformer decoder, still the entire input sequence is received in one go. But, when making preditions, it\n",
    "                uses masking to hide future parts of the input sequence.\n",
    "                <figure <figure style=\"text-align: center;\">>\n",
    "                    <img src=\"images/transformer_decoder.png\" />\n",
    "                </figure>\n",
    "                (Note the masking in what is sometimes called a causal self-attention layer.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>An example of a transformer decoder word-level language model is GPT-1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Generation</h1>\n",
    "<ul>\n",
    "    <li>An obvious use for a Language Model is text generation.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating text using the character-level language model</h2>\n",
    "<ul>\n",
    "    <li>To generate text, we want to make repeated predictions:\n",
    "        <ul>\n",
    "            <li>Feed in some initial input;</li>\n",
    "            <li>Predict the most likely next character/word;</li>\n",
    "            <li>Add the prediction to the end of the input text;</li>\n",
    "            <li>Feed in the extended input;</li>\n",
    "            <li>Predict the most likely next character/word;</li>\n",
    "        </ul>\n",
    "        and so on.\n",
    "    </li>\n",
    "    <li>But this results in output text that is very repetitive.</li>\n",
    "    <li>Instead, we make it stochastic:\n",
    "        <ul>\n",
    "            <li>We pick the next character/word randomly but based on the probabilities that the network produces.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We'll illustrate using the character-level RNN.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modification of the function for predicting the next character.\n",
    "# The temperature parameter allows you to tune it: \n",
    "# - a value close to zero favours high probability characters, but leads to more repetition\n",
    "# - a high value gives all characters an almost equal probability\n",
    "def predict_next_char(text, temperature=1):\n",
    "    x = np.array(vectorization_layer([text])) - 2\n",
    "    probabilities = char_language_model.predict(x, verbose=0)[0, -1:]\n",
    "    rescaled_logits = tf.math.log(probabilities) / temperature\n",
    "    predicted_token = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    predicted_char = vectorization_layer.get_vocabulary()[predicted_token + 2]\n",
    "    return predicted_char\n",
    "\n",
    "# A function that predicts the next character repeatedly\n",
    "def generate_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += predict_next_char(text, temperature)\n",
    "    return ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temperature:\n",
      " Alas with her to the duke of your sight of the state to \n",
      "------------------------------------------------------------\n",
      "Medium temperature:\n",
      " Alas my am your honour so to be?\n",
      "i love thee, as tender \n",
      "------------------------------------------------------------\n",
      "High temperature:\n",
      " Alas to: hish in uomagier,\n",
      "in impripsanmen ofrugled,\n",
      "im \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Some examples\n",
    "print(\"Low temperature:\\n\", generate_text(text=\"Alas \", temperature=0.2), \"\\n------------------------------------------------------------\")\n",
    "\n",
    "print(\"Medium temperature:\\n\", generate_text(text=\"Alas \", temperature=0.9), \"\\n------------------------------------------------------------\")\n",
    "\n",
    "print(\"High temperature:\\n\", generate_text(text=\"Alas \", temperature=2.0), \"\\n------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Not bad for such a small dataset, so few layers and so few training epochs!</li>\n",
    "</ul>\n",
    "<!--\n",
    "<ul>\n",
    "    <li>How can we make the generated text more convincing?\n",
    "        <ul>\n",
    "            <li>Tweak everything! More data, more layers, more neurons per layer, more epochs, &hellip;\n",
    "            <li>You could make the windows bigger by increasing <code>n_steps</code> but even LSTM and GRUs,\n",
    "                while better than SimpleRNNs, cannot handle very long sequences.\n",
    "            </li>\n",
    "            <li>We could change Char-RNN from being a <b>stateless RNN</b> to being a <b>stateful RNN</b>; see the Appendix.\n",
    "            </li>\n",
    "            <li>And, of course, we could use a transformer instead of an RNN, especially if we want a word-level model.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h2>Stateless RNNs and Stateful RNNs</h2>\n",
    "<ul>\n",
    "    <li><b>Stateless RNN:</b> In a training iteration, \n",
    "        <ul>\n",
    "            <li>will be trained on a batch of random chunks of the text;</li>\n",
    "            <li>hidden state starts at all zeros;</li>\n",
    "            <li>processes the input, step by step;</li>\n",
    "            <li>after the last timestep, throws away the hidden state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Stateful RNN:</b>\n",
    "        <ul>\n",
    "            <li>preserve the hidden state at the end of the last timestep;</li>\n",
    "            <li>use it as the initial hidden state for the next batch.</li>\n",
    "        </ul>\n",
    "        This way, we can learn longer patterns despite only back-propagating through short\n",
    "        sequences.\n",
    "    </li>\n",
    "    <li>However, we now must arrange our batches quite carefully.\n",
    "        <ul>\n",
    "            <li>Each input sequence in a batch starts where the corresponding sequence in the previous\n",
    "                batch finished.\n",
    "            </li>\n",
    "            <li>In other words, we must remove the overlapping and the shuffling that we used in the\n",
    "                stateless RNN.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Keras comes with a parameter for its recurrent layers, <code>stateful=True</code>.</li>\n",
    "</ul>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating sequences of things other than words or characters</h2>\n",
    "<ul>\n",
    "    <li>We can build language models on many different kinds of sequence data &mdash; not just human language data.</li>\n",
    "    <li>For example, we could train a language model on a music dataset &mdash; and use it to generate music!\n",
    "        <ul>\n",
    "            <li>Here's an example (one that uses a RNN): <a href=\"https://folkrnn.org/\">https://folkrnn.org/</a\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Of for example, we could train a language model on a dataset that contains sequences of brushstrokes &mdash; and use it to generate drawings!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating things other than sequences</h2>\n",
    "<ul>\n",
    "    <li>We've seen how language models can be used to generate sequences such as text or music.</li>\n",
    "    <li>Generating sequences is just one example of what has become known as <b>generative AI</b>.</li>\n",
    "    <li>You are, no doubt, familiar with other examples of AI to generate images, audio or video.\n",
    "    <!--\n",
    "        <ul>\n",
    "            <li><a href=\"https://ml4a.net/\">ml4a: Machine Learning for Art</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    -->\n",
    "    <li>Unfortunately, we have not time to discuss these. If you are interested in how they work, you can investigate the following:\n",
    "        <ul>\n",
    "            <li>Generative Adversarial Networks (GANs);</li>\n",
    "            <li>Variational encoders; and</li>\n",
    "            <li>Diffusion models and, more recently, stable diffusion.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sequence Modeling</h1>\n",
    "<ul>\n",
    "    <li>Let's review some applications that use sequence data. We will group them, depending on whether they are many-to-one, one-to-many or many-to-many. (Some people use different terminology: sequence-to-vector or seq2vec; vector-to-sequence or vec2seq; and sequence-to-sequence or seq2seq.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Many-to-one</h2>\n",
    "<ul>\n",
    "    <li>The input data is a sequence but the output is not; the output might be a fixed-size vector or a scalar.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/seq2vec.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Examples:\n",
    "        <ul>\n",
    "            <li>Timeseries forecasting, taking in sequences of numbers (historical rainfall data, stock prices, sales) and predicting the next number (tomorrow's rainfall, stock price, sales);</li>\n",
    "            <li>Sentiment analysis, taking in text, outputting a class (e.g. positive, neutral, negative) or a score;\n",
    "            <li>Activity recognition from video, taking in a video (sequence of images), outputting a class (e.g. running, walking, crawling);</li>\n",
    "            <li>Image generation, outputting an image that corresponds to the user's natural language description. Examples include OpenAI's <a href=\"https://openai.com/dall-e-2\">Dalle-2</a>, Google's <a href=\"https://imagen.research.google/\">Imagen</a> and\n",
    "                <a href=\"https://mid-journey.ai/\">MidJourney.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We can implement these models using RNNs (the last recurrent layer would say <code>return_sequences=False</code>) or using transformer encoders.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One-to-many</h3>\n",
    "<ul>\n",
    "    <li>The input data is not a sequence, but the output is.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/vec2seq.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Example\n",
    "        <ul>\n",
    "            <li>Image captioning, taking in an image, outputting a descriptive phrase (sequence of words).</li>\n",
    "        </ul\n",
    "    </li>\n",
    "    <li>We can implement these models using RNNs (the recurrent layers will all say <code>return_sequence=True</code>) or using a transformer decoder.\n",
    "        <ul>\n",
    "            <li>Here, for example, is what Google's image captioning system looked like some years ago.\n",
    "            <figure style=\"text-align: center\";>\n",
    "                <img src=\"images/captioning.png\" />\n",
    "                <figcaption>\n",
    "                    Google's image captioning system<br /> See\n",
    "                    <a href=\"https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html\">https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html</a><br />\n",
    "                    Image comes from Vinyals et al.: <i>Show and Tell: Lessons learned from the \n",
    "                    2015 MSCOCO Image Captioning Challenge</i>, CoRR, abs/1609.06647, 2016 \n",
    "                    (<a href=\"https://arxiv.org/pdf/1609.06647.pdf\">https://arxiv.org/pdf/1609.06647.pdf</a>)\n",
    "                </figcaption>\n",
    "            </figure>\n",
    "            A convolutonal neural network processes the image and passes a representation to a RNN that generates the caption.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Many-to-many</h3>\n",
    "<ul>\n",
    "    <li>Both the input and the output are sequences.</li>\n",
    "    <li>In some cases, many-to-many models work in a synchronized way: there is an output for each element of the input sequence.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/seq2seq.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Examples:\n",
    "        <ul>\n",
    "            <li>Video analysis, where each frame in the video is classified.</li>\n",
    "            <li>DNA analysis, taking in a DNA sequence, outputting a corresponding sequence but highlighting a subsequence within the \n",
    "                input that codes a certain protein, for example AGCCCCTGTGAGGAACTAG $\\rightarrow$ 0011111111111111100\n",
    "            </li>\n",
    "            <li>Named entity recognition, taking in text (sequence of words), outputting a corresponding sequence but highlighting\n",
    "                subsequences that are the names of people, organizations, etc. \"Elon Musk works for SpaceX and Tesla Inc.\" \n",
    "                $\\rightarrow$ 11001011. Named entity recognition has all sorts of uses. It might be used by search engines when \n",
    "                indexing documents, or by customer support departments when routing your queries and complaints &mdash; to give just \n",
    "                two examples. Rather than outputting bit strings (0s and 1s), it can be more useful if the system outputs symbols that \n",
    "                show where a name starts and where it ends. (There are, by the way, lots of other ways to build named entity recognition\n",
    "                systems, without using sequence models.)\n",
    "            </li>\n",
    "            <li>Part-of-speech tagging, taking in a sequence of words, outputting the grammatical category of each word. For example,\n",
    "                if the input is \"the cat sat on the mat\", the output is \"adjective noun verb preposition adjective noun\".\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But, for many tasks, synchronization would be wrong. Consider machine translation: \"La plume de ma tante est sur la table\"\n",
    "        $\\rightarrow$ \"My aunt's pen is on the table\"; \"vin rouge\" $\\rightarrow$ \"red wine\". Word-to-word translation from one language \n",
    "        to another would not give correct results.\n",
    "    </li>\n",
    "    <li>For these tasks, a better architecture is an encoder-decoder architecture:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/encoder_decoder.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>The encoder takes in the input sequence; its final output tries to capture the entire input sequence in a \n",
    "                single vector.\n",
    "            </li>\n",
    "            <li>That vector is passed to the decoder, which generates the output sequence.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In the <i>Attention is all you need</i> paper, transformers were presented as an encoder-decoder architecture:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/transformer_encoder_decoder.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Machine translation is not the only application that benefits from an encoder-decoder architecture.\n",
    "        <ul>\n",
    "            <li>Document paraphrasing: The input is a document; the output is also a document &mdash; one that covers the same ground but does so using different language.</li>\n",
    "            <li>Document summarization: The input is a document; the output is a shorter text that expresses the essence of the original document.</li>\n",
    "            <li>Question-answering: The input is a question; the output is a suitable answer.</li>\n",
    "            <li>Chatbots: The input may be the history of the conversation so far; \n",
    "                and the output is a suitable next contribution to the conversation.</li>\n",
    "            <li>Playlist captioning: The input is a sequence of songs. The encoder passes a representation of\n",
    "                those songs to the decoder, which generates a natural language description of the playlist.\n",
    "            </li>\n",
    "            <li>Text-to-melody and melody-to-melody, offered by Microsoft-funded <a href=\"https://www.audiogen.co/\">Audiogen</a> and by\n",
    "                Google's <a href=\"https://google-research.github.io/seanet/musiclm/examples/\">MusicLM</a>.\n",
    "            </li>\n",
    "            <li>Text-to-video, which is one of the capabilities of <a href=\"https://runwayml.com/\">Runway</a>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"https://quillbot.com/\">Quillbot</a> is one among many tools for paraphrasing, summarizing, and lots more.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
