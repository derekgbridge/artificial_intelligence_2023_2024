{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Recommender Systems III</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br />\n",
    "    School of Computer Science and Information Technology<br />\n",
    "    University College Cork\n",
    "</h2>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Warning</h1>\n",
    "<ul>\n",
    "    <li>Same warning: The code in this lecture is for educational purposes only &mdash; written for clarity (I hope). There is no attempt to achieve efficiency or robustness.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"../datasets/ml_movies.txt\", delimiter=\"|\", encoding=\"ISO-8859-1\",\n",
    "            names = [\"item_id\", \"title\", \"release date\", \"video release date\",\n",
    "                \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "                \"Children\\'s\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "                \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\",\n",
    "                \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]).drop([\n",
    "                \"release date\", \"video release date\",\n",
    "                \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "                \"Children\\'s\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "                \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\",\n",
    "                \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"], axis=1)\n",
    "movies[\"item_id\"] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"ml_ratings.txt\", delimiter=\"\\t\", encoding=\"ISO-8859-1\",\n",
    "                names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"]).drop(\"timestamp\", axis=1)\n",
    "ratings[\"user_id\"] -= 1\n",
    "ratings[\"item_id\"] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_matrix = ratings.pivot(index=\"user_id\", columns=\"item_id\", values=\"rating\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = ratings[\"user_id\"].unique()\n",
    "item_ids = ratings[\"item_id\"].unique()\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "num_ratings = len(ratings)\n",
    "mean = ratings[\"rating\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Matrix Factorization</h1>\n",
    "<ul>\n",
    "    <li>We continue to look at collaborative filtering.</li>\n",
    "    <li>In the previous lecture, we saw an instance-based approach to collaborative filtering:\n",
    "        user-based nearest-neighbours.\n",
    "    </li>\n",
    "    <li>In this lecture, we will look at a model-based approach to collaborative filtering: matrix factorization.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Embeddings</h2>\n",
    "<ul>\n",
    "    <li>Consider a ratings matrix, $\\v{R}$:\n",
    "        <table style=\"border: 1px solid; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\"></th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_1$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_2$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_3$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_4$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_5$</th>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$i_6$</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_1$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">1</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_2$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_3$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_4$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">3</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid black; text-align: left;\">$u_5$</th>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">2</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">5</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\">4</td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "                <td style=\"border: 1px solid black; text-align: left;\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>In $\\v{R}$,\n",
    "        <ul>\n",
    "            <li>each user is represented by a row vector of ratings with dimension $|I|$; and</li>\n",
    "            <li>each item is represented by a (column) vector of ratings with dimension $|U|$.</li>\n",
    "        </ul>\n",
    "        These vectors have a high dimension and are sparse.\n",
    "    </li>\n",
    "    <li>So why not come up with <b>embeddings</b>:\n",
    "        <ul>\n",
    "            <li>These would map the high dimensional sparse vectors to low-dimensional dense vectors. \n",
    "                (This should sound familiar!)\n",
    "            </li>\n",
    "        </ul>\n",
    "        But we will map users and items to the same space.\n",
    "        <ul>\n",
    "            <li>In other words, they will map to vectors that have the same dimension, call it $d$.</li>\n",
    "            <li>Each element represents a feature.</li>\n",
    "            <li>In the case of user embeddings, the values indicate how much the user likes that feature.</li>\n",
    "            <li>In the case of item embeddings, the values indicate how much the item possesses that feature.</li>\n",
    "        </ul>\n",
    "        So, let the embedding for user $u$ be a row vector of dimension $d$ and refer to it as $\\v{P}^{(u)}$.\n",
    "        And let the embedding for item $i$ be a (column) vector of dimension $d$ and refer to it as $\\v{Q}^{(i)}$.\n",
    "    </li>\n",
    "    <li>Predicting $\\hat{r}_{ui}$.\n",
    "        <ul>\n",
    "            <li>Ley $\\mu$ be the mean of all the ratings in $\\v{R}$.</li>\n",
    "            <li>We can predict $\\hat{r}_{ui}$ by computing the product of the user embedding and item embedding and adding this product to the mean:\n",
    "                $$\\hat{r}_{ui} = \\mu + \\v{P}^{(u)}\\v{Q}^{(i)}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Example for $d=3$ and $\\mu=3.5$:\n",
    "        <ul>\n",
    "            <li>Consider $u_2$.\n",
    "                <ul>\n",
    "                    <li>Her ratings are $\\rv{5,5,\\bot,3,4,\\bot}$.</li>\n",
    "                    <li>Suppose the corresponding embedding is $\\rv{0.8,0.4,-0.75}$ (never mind where this comes\n",
    "                        from for the moment).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>And consider $i_3$.\n",
    "                <ul>\n",
    "                    <li>Its ratings are $\\cv{5\\\\\\bot\\\\\\bot\\\\3\\\\4}$</li>\n",
    "                    <li>Suppose the corresponding embedding is $\\cv{0.0\\\\0.5\\\\0.3}$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Then the predicted rating of $u_2$ for $i_3$ is $\\mu$ plus the product of the two embeddings, i.e.\n",
    "                $$3.5 + 0.8\\times0.0 + 0.4\\times0.5 + -0.75\\times0.3 = 3.475$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Latent features</h2>\n",
    "<ul>\n",
    "    <li>The explanation should so far be very reminiscent of the simple content-based recommender from two lectures ago.</li>\n",
    "    <li>Things that feel the same:\n",
    "        <ul>\n",
    "            <li>Users and items are represented by vectors of dimension $d$ in the same space.</li>\n",
    "            <li>In the case of users, we can think of the values as being how much a user likes a feature.</li>\n",
    "            <li>In the case of items, we can think of the values as being how much an item possesses that feature.</li>\n",
    "            <li>Predictions involve computing the product of two vectors.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Things that feel different:\n",
    "        <ul>\n",
    "            <li>Previously, we <em>designed</em> the features, e.g. movie genres.\n",
    "                But here, the features and the values are <em>learned</em> from the ratings data. We refer to these\n",
    "                features as <b>latent features</b>, reflecting the idea that they are somehow hidden (latent) in\n",
    "                the ratings data and that we are revealing them through a learning algorithm.\n",
    "            </li>\n",
    "            <li>Previously, the product was to be thought of as measuring the similarity of the user and item.\n",
    "                Here, it is a predicted rating, and this is how we learn the features.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning the embeddings</h2>\n",
    "<ul>\n",
    "    <li>We need to learn all the user embedings $\\v{P}$ and all the item embeddings $\\v{Q}$. How?</li>\n",
    "    <li>We need a loss function. We can use, e.g., MSE.\n",
    "        $$J(\\v{P}, \\v{Q}) = \\frac{1}{|\\Omega|}\\sum_{\\langle u,i,r\\rangle \\in \\Omega} (\\mu + \\v{P}^{(u)}\\v{Q}^{(i)} - r)^2$$\n",
    "        Here $\\v{P}^{(u)}$ is the user embedding for user $u$ (which is one of the rows in $\\v{P}$) and $\\v{Q}^{(i)}$ is the item embedding for item $i$ (which is one of the columns in $\\v{Q}$). And recall that $\\Omega$ is the set of ratings in $\\v{R}$ that are not $\\bot$.\n",
    "    </li>\n",
    "    <li>(By the way, the loss function in this case is not convex.)\n",
    "    </li>\n",
    "    <li>Then, we can use, for example, Gradient Descent.</li>\n",
    "    <li>This, for example, is Stochastic Gradient Descent:\n",
    "        <ul style=\"background: lightgrey;\">\n",
    "            <li>initialise $\\v{P}$ and $\\v{Q}$ randomly</li>\n",
    "            <li>repeat until convergence\n",
    "                <ul>\n",
    "                    <li>repeat $|\\Omega|$ times\n",
    "                        <ul>\n",
    "                            <li>select $\\langle u,i,r\\rangle$ from $\\Omega$ at random</li>\n",
    "                            <li>$\\mathit{prediction} = \\mu + \\v{P}^{(u)}\\v{Q}^{(i)}$</li>\n",
    "                            <li>$\\v{P}^{(u)} \\gets \\v{P}^{(u)} - \\alpha \\times (\\mathit{prediction} - r) \\times \\v{Q}^{(i)}$</li>\n",
    "                            <li>$\\v{Q}^{(i)} \\gets \\v{Q}^{(i)} - \\alpha \\times (\\mathit{prediction} - r) \\times \\v{P}^{(u)}$</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(d, alpha, num_epochs):\n",
    "    \n",
    "    P = np.random.rand(num_users, d)\n",
    "    Q = np.random.rand(num_items, d)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_items):\n",
    "                r = ratings_matrix.loc[i][j]\n",
    "                if r != 0.0:\n",
    "                    eij = mean + np.dot(P[i,:],Q[j,:]) - r\n",
    "                    P[i] = P[i] - alpha * eij * Q[j]\n",
    "                    Q[j] = Q[j] - alpha * eij * P[i]\n",
    "                    \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_by_mf(user_id, item_id):\n",
    "    return mean + np.dot(P[user_id,:],Q[item_id,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Q = matrix_factorization(d=10, alpha=0.001, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5830181424032435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rating_by_mf(user_id=13, item_id=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Matrix completion</h2>\n",
    "<ul>\n",
    "    <li>In the presentation of this material above, we concentrated on individual predictions, $\\mu + \\v{P}^{(u)}\\v{Q}^{(i)}$.</li>\n",
    "    <li>But, if we multiply matrices $\\v{P}$ and $\\v{Q}$, $\\v{P}\\v{Q}$, and add $\\mu$ element-wise, $\\mu + \\v{P}\\v{Q}$, then we get all predictions at once!</li>\n",
    "    <li>This is why some people refer to this as <b>matrix completion</b>: we're getting predictions for all the\n",
    "        entries that are $\\bot$ (and all those that are not $\\bot$).\n",
    "    </li>\n",
    "    <li>It is also why this is <b>matrix factorization</b> (i.e. factorization is writing one thing as a product of other things).\n",
    "        What we are doing is finding $\\v{P}$ and $\\v{Q}$, two lower-rank matrices, such that\n",
    "        $$\\mu + \\v{P}\\v{Q} \\approx \\v{R}$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Discussion of matrix factorization</h2>\n",
    "<ul>\n",
    "    <li>Advantages of matrix factorization for collaborative filtering include:\n",
    "        <ul>\n",
    "            <li>It does not require any item or user descriptions, just user-item interactions (e.g. ratings) &mdash;\n",
    "                and this is data we will collect during the normal operation of the system.\n",
    "            </li>\n",
    "            <li>It may recommend items that are pleasantly surprising (certainly more so than content-based\n",
    "                approaches), since it recommends using <em>other peoples'</em> tastes.\n",
    "            </li>\n",
    "            <li>It is fast at prediction time.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Its disadvantages include:\n",
    "        <ul>\n",
    "            <li>Learning the embeddings (the SGD above) takes time. So new ratings generally cannot take \n",
    "                immediate effect. They will have to be buffered until the next time the model gets updated\n",
    "                (e.g. every night). (There are, however, some incremental versions of matrix factorization,\n",
    "                which do allow new ratings to take immediate effect.)\n",
    "            </li>\n",
    "            <li>It has problems recommending to cold-start users and recommending cold-start items.</li>\n",
    "            <li>It can exhibit popularity bias: over-recommending popular items (although this may depend to\n",
    "                some extent on details of the implementation).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Can it explain its recommendations?\n",
    "        <ul>\n",
    "            <li>The basic answer is, No. The latent features do not mean anything to human users. (Of course, there\n",
    "                is research that tries to constrain the latent features in various ways to try to make them more\n",
    "                human-interpretable.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In concluding, let's mention some variants.\n",
    "        <ul>\n",
    "            <li>It is normal to use slightly more complicated formulae to learn values referred to as\n",
    "                user biases and item biases&mdash; these help deal with the problems with ratings scales that we\n",
    "                mentioned in the previous lecture.\n",
    "            </li>\n",
    "            <li>It is normal to use regularization.</li>\n",
    "            <li>There are variants that combine with nearest-neighbours in various ways.\n",
    "                <!-- E.g. learn embeddings then run user-based knn on these instead of the ratings vectors.\n",
    "                     E.g. learn the similarities of a usr-based knn model in a way similar to MF.\n",
    "                     E.g. combine the previous one with MF.\n",
    "                  -->\n",
    "            </li>\n",
    "            <li>There are variants that allow item descriptions, user descriptions and contextual information\n",
    "                to be added to $\\v{R}$ in various ways, thus giving a system that handles all the kinds of\n",
    "                data that we may have.\n",
    "                <!-- E.g. block 00 is R, block 01 adds user descriptions as extra columns, block 10 adds item\n",
    "                     descriptions as extra rows, block 11 is not used, then factorize.\n",
    "                     E.g. factorization machines.\n",
    "                     And so on.\n",
    "                 -->\n",
    "            </li>\n",
    "            <li>Standard matix factorization is actually a linear model! We can use the idea of user and item embeddings in, for example, neural networks, to give nonlinear models.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Top-N Recommendation</h1>\n",
    "<ul>\n",
    "    <li>Recall that recommender systems typically proceed through (at least) three steps:\n",
    "        <figure>\n",
    "            <img src=\"images/rs_arch.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Let's select the top-$N$ candidates, this time using matrix factorization.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>668</td>\n",
       "      <td>Body Parts (1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>Madness of King George, The (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>360</td>\n",
       "      <td>Incognito (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Terminator 2: Judgment Day (1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>Twister (1996)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id                               title\n",
       "668      668                   Body Parts (1991)\n",
       "51        51  Madness of King George, The (1994)\n",
       "360      360                    Incognito (1997)\n",
       "95        95   Terminator 2: Judgment Day (1991)\n",
       "117      117                      Twister (1996)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 11\n",
    "N = 5\n",
    "\n",
    "# Get the item_ids sorted by predicted rating\n",
    "sorted_item_ids = np.argsort(\n",
    "        [predict_rating_by_mf(user_id=user_id, item_id=i_id) \n",
    "         for i_id in item_ids]).tolist()\n",
    "# To be a recommendation, the user must not have watched this movie \n",
    "taboo_item_ids = ratings_matrix.loc[user_id][ratings_matrix.loc[user_id] == 0.0].tolist()\n",
    "# Get the item_ids for the top-N recommendations\n",
    "recommended_item_ids = [i_id for i_id in sorted_item_ids if i_id not in taboo_item_ids][:N]\n",
    "# Get the titles of these movies\n",
    "movies.loc[recommended_item_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Selecting the $N$ candidates whose scores are highest and recommending these to the user is just the obvious thing.</li>\n",
    "    <li>However, there may be some additional criteria to take into account at this stage. Some examples include:\n",
    "        <ul>\n",
    "            <li>There may be some business rules to take into account. For example, there may be some items the business\n",
    "                is trying to push. So there may be a rule that requires that one or more slots in the top-$N$ are occupied by these items, displacing the 'organic' recommendations. (Think about sponsored content, for example.)\n",
    "            </li>\n",
    "            <li>We might have more than one recommender model whose scores we want to combine.</li>\n",
    "            <li>We often carry out some re-ranking at this stage in order to ensure that the top-$N$ has a \n",
    "                degree of diversity or some notion of fairness.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let's look at just one example: diversity.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Top-$N$ Diversity</h2>\n",
    "<ul>\n",
    "    <li>Suppose the user likes fantasy and thrillers and comedies. And suppose we recommend the $N$ candidate \n",
    "        items that obtained the highest scores (highest predicted rating\n",
    "        in our case).\n",
    "        <ul>\n",
    "            <li>Maybe this is what we end up recommending:\n",
    "                <figure>\n",
    "                    <img src=\"images/top-N.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Each recommendation is <em>relevant</em> to the user. She likes fantasy! But this top-$N$ lacks\n",
    "        <b>diversity</b>.\n",
    "        <ul>\n",
    "            <li>A more diverse top-$N$ (e.g. containing at least one thriller, at least one comedy)\n",
    "                would be more likely to include at least one recommendation that would\n",
    "                satisfy the user.\n",
    "            </li>\n",
    "            <li>It would give her a meaningful choice.</li>\n",
    "            <li>It would be one way of handling the recommender's uncertainty about the user's preferences.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defining diversity</h2>\n",
    "<ul>\n",
    "    <li>Diversity is a property of a set of items, not a property of an individual item.</li>\n",
    "    <li>Suppose we have a set of recommendations $S$. We can measure the <em>marginal</em>\n",
    "        increase in diversity obtained by adding item $i$ into set $S$ as the maximum\n",
    "        distance (smallest similarity) between $i$ and the members of $S$:\n",
    "        $$div(i, S) = max_{j \\in S}(1 - sim(i, j))$$\n",
    "    </li>\n",
    "    <li>E.g. if we add another Star Wars movie to the top-$N$ shown previously, it is very similar\n",
    "        to the ones we have already, so its marginal diversity is very low.\n",
    "    </li>\n",
    "    <li>But if we add a comedy to the top-$N$, then it is not very similar to the movies we have\n",
    "        already, so its marginal diversity is very high.\n",
    "    </li>\n",
    "    <li>Similarity can be measured in any of the ways we have discussed already, e.g. cosine or\n",
    "        Pearson of vectors whose features are genres or ratings or latent features.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Greedy re-ranking</h2>\n",
    "<ul>\n",
    "    <li>So, instead of recommending the $N$ with the highest predicted ratings, we select the\n",
    "        $N$ that achieve the best balance between relevance and diversity, controlled by a hyperparameter\n",
    "        $\\lambda \\in [0, 1]$:\n",
    "        <ul style=\"background: lightgrey;\">\n",
    "            <li>$S \\gets [\\,\\,]$</li>\n",
    "            <li>while $|S| < N$\n",
    "                <ul>\n",
    "                    <li>$i^* \\gets \\arg\\max_{i \\in candidates} \\hat{r}_{ui} + \\lambda div(i,S)$</li>\n",
    "                    <li>delete $i^*$ from candidates</li>\n",
    "                    <li>append $i^*$ to the end of $S$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>return $S$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>As usual, there are lots of variations on this, especially lots of different ways of defining\n",
    "        diversity.\n",
    "    </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
