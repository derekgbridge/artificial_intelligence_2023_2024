{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Accuracy Estimation</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Error Estimation for Classifiers</h1>\n",
    "<ul>\n",
    "    <li>Recall we want to know how well a model will do in practice, once we start to use it to make predictions.\n",
    "        <ul>\n",
    "            <li>For regression, this is called <b>error estimation</b>. \n",
    "            </li>\n",
    "            <li>For classification, you can call it 'error estimation', but we also call it\n",
    "                <b>accuracy estimation</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We discussed the importance of doing error estimation on a separate test set.</li>\n",
    "    <li>We also saw that the performance measure we use for error estimation might be different\n",
    "        from the loss function we used when training the model.\n",
    "        <ul>\n",
    "            <li>For regression, the loss function is often MSE, but we might use MAE (or RMSE)\n",
    "                for error estimation.\n",
    "            </li>\n",
    "           <li>For classification, the loss function is often cross-entropy (see later lecture), but we might use\n",
    "               <b>accuracy</b> for error estimation.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Accuracy</h1>\n",
    "<ul> \n",
    "    <li>Accuracy is the obvious performance measure for a classifier: the ratio of the number of correct \n",
    "        predictions to the number of predictions made.\n",
    "    </li>\n",
    "    <li>If you like notation:\n",
    "        $$\\frac{1}{|T|}\\sum_{i = 1}^{|T|} I(\\hat{y}^{(i)} = y^{(i)})$$\n",
    "        where $T$ is the set of examples on which you are testing the classifier and hence $|T|$ is \n",
    "        the number of examples, and $I(p)$ is the indicator function that outputs 1\n",
    "        if predicate $p$ is true and zero otherwise.\n",
    "    </li>\n",
    "    <li>There are, however, numerous other measures that we could use instead, or as well. If we had more time,\n",
    "        we might study precision, recall, F1, Area Under the Curve (AUC), and lots more.\n",
    "        However, beware of 'fishing expeditions':\n",
    "        <ul>\n",
    "            <li>You should choose a measure or two in advance of running any experiments &mdash; \n",
    "                the measure(s) that you think are best-aligned to your business problem.\n",
    "            </li>\n",
    "            <li>When there are many measures, there is a temptation to calculate them all and then get excited when\n",
    "                the learner performs well on one of them.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Limitations of Classification Accuracy</h2>\n",
    "<ul>\n",
    "    <li>Accuracy summarizes overall performance in a single figure, which is a good thing.</li>\n",
    "    <li>But it has at least three problems:   \n",
    "        <ol>\n",
    "            <li>\n",
    "                Giving only a single figure hides information about how the classifier performs on the individual \n",
    "                classes.\n",
    "                <ul>\n",
    "                    <li>This problem becomes more acute when the costs of different kinds of misclassification \n",
    "                        are not equal.\n",
    "                    </li>\n",
    "                    <li>For example, in email classification, it is more serious to misclassify ham as spam.\n",
    "                    </li>\n",
    "                    <li>In principle, we could assign costs to the different kinds of mis-classification and define\n",
    "                        a cost-sensitive variant of classification accuracy, but, in practice, it's difficult, if\n",
    "                        not impossible, to come up with the costs; for example, how much worse is it to \n",
    "                        classify ham as spam than spam as ham?\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Classification accuracy is also best when the distribution of classes in $T$ is reasonably\n",
    "                balanced.\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        If, on the other hand, some of the classes are more prevalent than others, then they \n",
    "                        tend to bias the measure, e.g. if you do well on the more prevalent classes, then you get \n",
    "                        a higher score overall.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Accuracy does not take into account correct classifications from mere chance.\n",
    "                <ul>\n",
    "                    <li>There are performance measures that <em>correct for chance</em>\n",
    "                        (but we won't look at them!)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>       \n",
    "    </li>\n",
    "    <li>Computing a <b>confusion matrix</b> can help with the first problem.</li>\n",
    "    <li>Comparing with a <b>majority-class classifier</b> can help with the second and third problems.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Confusion Matrix</h1>\n",
    "<ul>\n",
    "    <li>\n",
    "        The <b>confusion matrix</b> $CM$ for a classifier is a square $|C| \\times |C|$ matrix.\n",
    "        <ul>\n",
    "            <li>In a confusion matrix, a cell $CM[i, j]$\n",
    "                contains the number of test examples of class $i$ that were classified as class $j$.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Here are examples of confusion matrices for a binary classifier and a multiclass classifier:\n",
    "        <div>\n",
    "            <table style=\"float: left; \">\n",
    "                <tr>\n",
    "                    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "                    <th colspan=\"2\">Predicted Class</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"border-bottom: 1px solid black\">0</th><th style=\"border-bottom: 1px solid black\">1</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"2\">Actual Class</th><th style=\"border-right: 1px solid black\">0</th><td>25</td><td>10</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"border-right: 1px solid black\">1</th><td>20</td><td>45</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th colspan=\"2\" rowspan=\"2\"></th><th colspan=\"3\">Predicted Class</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"border-bottom: 1px solid black\">0</th><th style=\"border-bottom: 1px solid black\">1</th><th style=\"border-bottom: 1px solid black\">2</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th rowspan=\"3\">Actual Class</th><th style=\"border-right: 1px solid black\">0</th><td>10</td><td>0</td><td>15</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"border-right: 1px solid black\">1</th><td>5</td><td>30</td><td>10</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th style=\"border-right: 1px solid black\">2</th><td>5</td><td>5</td><td>20</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </li>\n",
    "    <li>Let's assume a test set $T$.\n",
    "        <ul>\n",
    "            <li>\n",
    "                The sum of all entries in $CM$ equals $|T|$.\n",
    "            </li>\n",
    "            <li>\n",
    "                The sum of the entries in <em>row</em> $i$ is\n",
    "                the number of examples in $T$ that have class $i$.\n",
    "            </li>\n",
    "            <li>\n",
    "                The sum of the entries in <em>column</em> $j$ is\n",
    "                the number of examples in $T$ that the classifier assigns to class $j$.\n",
    "            </li>\n",
    "            <li>\n",
    "                Entries on the main diagonal $CM[i,i]$, are correctly classified, and so $\\sum_i CM[i,i]$ is\n",
    "                the total number of correctly classified examples.\n",
    "            </li>\n",
    "            <li>\n",
    "                Entries off the main diagonal, $CM[i, j], i\\neq j$, are incorrectly classified, and so \n",
    "                $\\sum_i\\sum_{j, j \\neq i} CM[i,j]$ is the total number of incorrectly classified examples.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>So, in a word, what does this calculate?\n",
    "        $$\\frac{\\sum_i CM[i,i]}{|T|}$$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Class exercise</h2>\n",
    "<ul>\n",
    "    <li>Classifier A and classifier B have the same classification accuracy (0.6) but which classifier would you use?</li>\n",
    "</ul>\n",
    "        <table style=\"float: left\">\n",
    "            <tr>\n",
    "                <th colspan=\"2\" rowspan=\"2\">Classifier A</th><th colspan=\"2\">Predicted Class</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border-bottom: 1px solid black\">Benign</th><th style=\"border-bottom: 1px solid black\">Malignant</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th rowspan=\"2\">Actual Class</th><th style=\"border-right: 1px solid black\">Benign</th><td>400</td><td>100</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th style=\"border-right: 1px solid black\">Malignant</th><td>300</td><td>200</td>\n",
    "            </tr>\n",
    "         </table>\n",
    "         <table>\n",
    "             <tr>\n",
    "                 <th colspan=\"2\" rowspan=\"2\">Classifier B</th><th colspan=\"2\">Predicted Class</th>\n",
    "             </tr>\n",
    "             <tr>\n",
    "                 <th style=\"border-bottom: 1px solid black\">Benign</th><th style=\"border-bottom: 1px solid black\">Malignant</th>\n",
    "             </tr>\n",
    "             <tr>\n",
    "                 <th rowspan=\"2\">Actual Class</th><th style=\"border-right: 1px solid black\">Benign</th><td>200</td><td>300</td>\n",
    "             </tr>\n",
    "             <tr>\n",
    "                 <th style=\"border-right: 1px solid black\">Malignant</th><td>100</td><td>400</td>\n",
    "             </tr>\n",
    "         </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Confusion Matrices in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn can produce a confusion matrix:\n",
    "        <ul>\n",
    "            <li>call <code>confusion_matrix</code>.</li>\n",
    "        </ul>\n",
    "        An example comes later.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Majority-Class Classifier</h1>\n",
    "<ul>\n",
    "    <li>\n",
    "        Consider $T$ that contains 950 positive examples and 50 negative examples.\n",
    "    </li>\n",
    "    <li>An extremely effective classifier in terms of accuracy for this $T$ is the so-called \n",
    "       <b>majority-class classifier</b>,\n",
    "       <ul>\n",
    "           <li>It <em>always</em> predicts the majority class.</li>\n",
    "       </ul>\n",
    "    <li>In this example, it predicts the positive class, and its accuracy is very high: 0.95.</li>\n",
    "    <li>But it isn't really a good classifier: it has no ability to discriminate between\n",
    "        positive and negative examples.\n",
    "    </li>\n",
    "    <li>Many people compare the accuracy of their classifier(s) against the \n",
    "        accuracy of the majority-class classifier to check that they are doing better\n",
    "        than this simple-minded <b>baseline</b>.\n",
    "        <ul>\n",
    "            <li>It is one way of partly overcoming the second and third problems on the earlier slide.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Majority-class classifier in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn has a<code>DummyClassifier</code> class.</li>\n",
    "    <li>An example comes later.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Exercise</h2>\n",
    "<ul>\n",
    "    <li>The majority-class classifier is a 'baseline' that we can compare against when evaluating a classifier.</li>\n",
    "    <li>Propose a baseline that you can compare against when evaluating a regressor.</li>\n",
    "    <li>Is it in scikit-learn?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stratification</h1>\n",
    "<ul>\n",
    "    <li>Holdout (and many other methods) rely on <em>randomly</em>\n",
    "        partitioning a dataset into a training set and a validation set and/or a test set\n",
    "        <ul>\n",
    "            <li>We've discussed before that the split may be 'lucky' or 'unlucky', hence resampling methods\n",
    "                such as $k$-Fold Cross-Validation.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But there is an additional problem in the case of classification.</li>\n",
    "    <li>In the case of classification, the split might not reflect\n",
    "        the distribution of examples within the classes:\n",
    "        <ul>\n",
    "            <li>Examples of one class might be under-represented in the training set or test set.</li>\n",
    "            <li>Examples of one class might even be completely absent from the training set or test set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Stratification</b> is the solution to this: \n",
    "        <ul>\n",
    "            <li>In stratification, the proportion of examples of each class in the overall dataset is respected \n",
    "                in the partitioning into training and validation/test sets\n",
    "            </li>\n",
    "            <li>Here's pseudocode for <b>stratified holdout</b>. For simplicity, the pseudocode only \n",
    "                covers the case of binary classification:\n",
    "                <ul style=\"background: lightgrey\">\n",
    "                    <li>Divide the dataset into positive examples, $P$, and negative examples, $N$</li>\n",
    "                    <li>Randomly partition $P$ into $\\mathit{Train}_P$ and $\\mathit{Test}_P$</li>\n",
    "                    <li>Randomly partition $N$ into $\\mathit{Train}_N$ and $\\mathit{Test}_N$</li>\n",
    "                    <li>$\\mathit{Train} \\gets \\mathit{Train}_P \\cup \\mathit{Train}_N$</li>\n",
    "                    <li>$\\mathit{Test} \\gets \\mathit{Test}_P \\cup \\mathit{Test}_N$</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Although this fixes the distribution with respect to the classes, you may still get 'lucky' or 'unlucky' \n",
    "        in other ways. So you will still want a large dataset for holdout. And if you don't have a large enough\n",
    "        dataset, then do the above multiple times, e.g. <b>Stratified \n",
    "        $k$-Fold Cross-Validation</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Stratification in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>For the <code>train_test_split</code> function, there is a <code>stratify</code> argument.</li>\n",
    "    <li>Corresponding to the <code>ShuffleSplit</code> class, there is a <code>StratfiedShuffleSplit</code>\n",
    "        class.\n",
    "    </li>\n",
    "    <li>Corresponding to the <code>KFold</code> class, there is a <code>StratifiedKFold</code> class.</li>\n",
    "    <li>If you use the shorthand for $k$-fold cross-validation (e.g. <code>cv=10</code>) then, for \n",
    "        classifiers, you will get stratified $k$-fold cross-validation.\n",
    "    </li>\n",
    "    <li>There are examples below.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case Study: the CS1109 Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_cs1109.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the test set: 20% of the dataset. Note the stratification\n",
    "dev_df, test_df = train_test_split(df, train_size=0.8, stratify=df[\"outcome\"], random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "features = [\"lect\", \"lab\", \"cao\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), features)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df[features]\n",
    "test_X = test_df[features]\n",
    "\n",
    "# Target values, encoded and converted to a 1D numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"outcome\"])\n",
    "dev_y = label_encoder.transform(dev_df[\"outcome\"])\n",
    "test_y = label_encoder.transform(test_df[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086956521739131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how a majority-class classifier performs.\n",
    "# This is our baseline. We need to do better than this!\n",
    "\n",
    "maj = DummyClassifier()\n",
    "maj.fit(dev_X, dev_y)\n",
    "accuracy_score(test_y, maj.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, note the use of scoring=\"accuracy\".\n",
    "# And note that cv=10 is automatically stratified k-fold cross-validation.\n",
    "\n",
    "# We'll just try kNN and LogisticRegression.\n",
    "# We could also do Logistic Regression with L1-regularization, Logistic Regression with \n",
    "# L2-regularization and any number of other classifiers to be found in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'predictor__n_neighbors': 10}, 0.7584656084656084)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsClassifier())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for kNN\n",
    "knn_param_grid = {\"predictor__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "knn_gs = GridSearchCV(knn, knn_param_grid, scoring=\"accuracy\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "knn_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "knn_gs.best_params_, knn_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7808201058201057"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with logistic regression\n",
    "logistic = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LogisticRegression())])\n",
    "\n",
    "# We should do a grid search to set hyperparameter C. But, for brevity, we won't!\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "np.mean(cross_val_score(logistic, dev_X, dev_y, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both are much better than the majority-class classifier.\n",
    "# But not much different from each other.\n",
    "# We can check for under/overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.7936535589845694\n",
      "Validation accuracy:  0.7584656084656084\n"
     ]
    }
   ],
   "source": [
    "knn.set_params(**knn_gs.best_params_) \n",
    "scores = cross_validate(knn, dev_X, dev_y, cv=10, \n",
    "                        scoring=\"accuracy\", return_train_score=True)\n",
    "print(\"Training accuracy: \", np.mean(scores[\"train_score\"]))\n",
    "print(\"Validation accuracy: \", np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn may be overfitting a little. So we'll reject kNN.\n",
    "# The other one does not overfit (not shown).\n",
    "# So let's evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8115942028985508"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-train Logistic Regression on train+validation and test on the test set\n",
    "\n",
    "logistic.fit(dev_X, dev_y)\n",
    "accuracy_score(test_y, logistic.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If we care about one kind of error more than another, we can look at a confusion matrix\n",
    "\n",
    "cm = confusion_matrix(test_y, logistic.predict(test_X), labels=logistic.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logistic.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Logistic Regression seems to predict better for people who pass.\n",
    "\n",
    "# Anyway, now decide whether to deploy.\n",
    "# If yes, then train on the entire dataset (not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Case Study: the Iris Dataset</h1>\n",
    "<ul>\n",
    "    <li>The famous Iris dataset was created by the even more (in)famous statistician, Ronald Fisher\n",
    "        (Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, \n",
    "        179-188, 1936).\n",
    "    </li>\n",
    "    <li>It is available all over the web now, especially the following dataset repository:\n",
    "        <a href=\"https://archive.ics.uci.edu/ml/datasets/iris\">https://archive.ics.uci.edu/ml/datasets/iris</a>\n",
    "    </li>\n",
    "    <li>But, we don't need to download it. scikit-learn has a copy and a function to load it.</li>\n",
    "    <li>Details:\n",
    "        <ul>\n",
    "            <li>$m = 150$ examples: each one is a flower &mdash; in fact, an Iris.</li>\n",
    "            <li>$n = 4$ features: sepal length, sepal width, petal length and petal width (all in centimetres).</li>\n",
    "            <li>Three classes (different kinds of Iris): <i>Iris setosa</i>, <i>Iris versicolor</i> and\n",
    "                <i>Iris virginica</i>\n",
    "                <img src=\"images/irises.png\" />\n",
    "            </li>\n",
    "            <li>Big warning: \n",
    "                <ul>\n",
    "                    <li>In most versions, the dataset is sorted: 50 setosa then 50 versicolor then 50 virginica.</li>\n",
    "                    <li>So shuffling is important.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (a dictionary) and get the features DataFrame and target values from the dictionary\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Shuffle the features and the target values in the same way\n",
    "idx = np.random.permutation(df.index)\n",
    "df.reindex(idx)\n",
    "y.reindex(idx)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
       "       'petal width (cm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks at class distribution\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the test set: 20% of the dataset.\n",
    "dev_df, test_df, dev_y, test_y = train_test_split(df, y, train_size=0.8, stratify=y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot two of the features and the class label\n",
    "\n",
    "plengths = df[\"petal length (cm)\"]\n",
    "pwidths = df[\"petal width (cm)\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.title(\"Petal width against petal length\")\n",
    "plt.scatter(plengths[y==0], pwidths[y==0], color = \"green\")\n",
    "plt.scatter(plengths[y==1], pwidths[y==1], color = \"red\")\n",
    "plt.scatter(plengths[y==2], pwidths[y==2], color = \"purple\")\n",
    "plt.xlabel(\"Petal length (cm)\")\n",
    "plt.xlim(0, 8)\n",
    "plt.ylabel(\"Petal width (cm)\")\n",
    "plt.ylim(0, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you notice from the diagram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), df.columns)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need a majority-class classifier.\n",
    "# We know that predicting one class all the time will get 1/3 correct.\n",
    "# This is our baseline. We need to do better than this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we'll compare different kinds of Logistic Regression:\n",
    "# - Multinomial Logistic Regression\n",
    "# - One-versus-rest Logistic Regression\n",
    "# - Multinomial Logistic Regression with L2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with multinomial logistic regression\n",
    "multinomial = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LogisticRegression(penalty=\"none\"))])\n",
    "\n",
    "# We don't need a grid search because we're not regularizing, so we don't have that hyperparameter.\n",
    "\n",
    "# Get the validation error\n",
    "np.mean(cross_val_score(multinomial, dev_df, dev_y, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with multinomial logistic regression\n",
    "ovr = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LogisticRegression(multi_class=\"ovr\"))])\n",
    "\n",
    "# We don't need a grid search because we're not regularizing, so we don't have that hyperparameter.\n",
    "\n",
    "# Get the validation error\n",
    "np.mean(cross_val_score(ovr, dev_df, dev_y, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'predictor__C': 0.3}, 0.9416666666666667)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline that combines the preprocessor with logistic regression with L2-regularization\n",
    "logisticL2 = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LogisticRegression(solver=\"saga\", penalty=\"l2\"))])\n",
    "\n",
    "# Create a dictionary of hyperparameters for ridge regression\n",
    "logisticL2_param_grid = {\"predictor__C\": [0.1, 0.3, 0.5],}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "logisticL2_gs = GridSearchCV(logisticL2, logisticL2_param_grid, scoring=\"accuracy\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "logisticL2_gs.fit(dev_df, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "logisticL2_gs.best_params_, logisticL2_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pick plain Multinomial Logistic Regression.\n",
    "\n",
    "# Re-train it on train+validation and test on the test set\n",
    "\n",
    "multinomial.fit(dev_df, dev_y)\n",
    "accuracy_score(test_y, multinomial.predict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now decide whether to deploy.\n",
    "# If yes, then train on the entire dataset (not shown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>For those of you whose machine learning work is not confined to this module, I recommend this paper, which\n",
    "        (happily) agrees with everything I've taught you and even goes a bit further:<br />\n",
    "        Michael A. Lones (2021): <i>How to avoid machine learning pitfalls: a guide for academic researchers</i>.\n",
    "        <a href=\"https://arxiv.org/abs/2108.02497\">https://arxiv.org/abs/2108.02497</a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
