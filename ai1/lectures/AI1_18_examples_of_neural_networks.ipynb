{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Neural Network Examples</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preliminaries</h1>\n",
    "<ul>\n",
    "    <li>The code below reads the CorkA, CS1109 and Iris datasets from CSV files to Panda DataFrames, shuffles them, splits them 80% train and 20% test, and converts to numpy arrays.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cork Property Prices Dataset\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "cork_df = pd.read_csv(\"../datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "cork_df = cork_df.sample(frac=1, random_state=2)\n",
    "cork_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "train_cork_df, test_cork_df = train_test_split(cork_df, train_size=0.8, random_state=2)\n",
    "\n",
    "# The features \n",
    "cork_features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract the features and convert to numpy array\n",
    "train_cork_X = train_cork_df[cork_features].values\n",
    "test_cork_X = test_cork_df[cork_features].values\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "train_cork_y = train_cork_df[\"price\"].values\n",
    "test_cork_y = test_cork_df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS1109 Dataset\n",
    "\n",
    "# Use pandas to read the CSV file into a DataFrame\n",
    "cs1109_df = pd.read_csv(\"../datasets/dataset_cs1109.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "cs1109_df = cs1109_df.sample(frac=1, random_state=2)\n",
    "cs1109_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "train_cs1109_df, test_cs1109_df = train_test_split(cs1109_df, train_size=0.8, random_state=2)\n",
    "\n",
    "# The features \n",
    "cs1109_features = [\"lect\", \"lab\", \"cao\"]\n",
    "\n",
    "# Extract the features and convert to a numpy array\n",
    "train_cs1109_X = train_cs1109_df[cs1109_features].values\n",
    "test_cs1109_X = test_cs1109_df[cs1109_features].values\n",
    "\n",
    "# Target values, encoded and converted to a 1D numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(cs1109_df[\"outcome\"])\n",
    "train_cs1109_y = label_encoder.transform(train_cs1109_df[\"outcome\"])\n",
    "test_cs1109_y = label_encoder.transform(test_cs1109_df[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris dataset\n",
    "\n",
    "# Load the dataset (a dictionary) and get the features DataFrame and target values from the dictionary\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# Shuffle the features and the target values in the same way\n",
    "idx = np.random.permutation(iris_df.index)\n",
    "iris_df.reindex(idx)\n",
    "iris_y.reindex(idx)\n",
    "iris_df.reset_index(drop=True, inplace=True)\n",
    "iris_y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "train_iris_df, test_iris_df, train_iris_y, test_iris_y = train_test_split(iris_df, iris_y, train_size=0.8, \n",
    "                                                                              random_state=4)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_iris_X = train_iris_df.values\n",
    "train_iris_y = train_iris_y.values\n",
    "test_iris_X = test_iris_df.values\n",
    "test_iris_y = test_iris_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>We'll use layered, dense, feedforward neural networks for regression, binary classification\n",
    "        and multi-class classification:\n",
    "        <ul>\n",
    "            <li>We'll use our three small datasets that, as we discussed before, contain <b>structured data</b> (sometimes\n",
    "                called <b>tabular data</b>): not necessarily ideal for deep learning.</li>\n",
    "            <li>We'll see one example that uses images.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This will illustrate some of the different activation functions we can use:\n",
    "        <ul>\n",
    "            <li>in the output layer: linear, sigmoid or softmax; and</li>\n",
    "            <li>in the hidden layers: sigmoid or ReLU.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This will also introduce the Keras library.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>The Keras library</h1>\n",
    "<ul>\n",
    "    <li>scikit-learn has very limited support for neural networks.</li>\n",
    "    <li>There are now many software frameworks that do support tensor computation, neural neworks and deep learning \n",
    "         including in Python:\n",
    "        <ul>\n",
    "            <li>TensorFlow, PyTorch and JAX are the main ones.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use Keras, which is a high-level API, first released in 2015\n",
    "        by Fran&ccedil;ois Chollet of Google (<a href=\"https://keras.io\">https://keras.io</a>), which has done\n",
    "        a lot to make Deep Learning accessible to people:\n",
    "        <ul>\n",
    "            <li>It is very high-level, making it easy to construct networks, fit models and make predictions.</li>\n",
    "            <li>It is multi-backend, meaning it works atop of whichever framework you prefer (TensorFlow, PyTorch or JAX).</li>\n",
    "            <li>The downside is it gives less fine-graned control than the backend framework itself. When fine-grained control is needed, you can mix in functions, methods and classes from the backend.</li>\n",
    "            <li>This seems a suitable trade-off for us: our module is about AI, not the intricacies of\n",
    "                TensorFlow, PyTorch or JAX.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Keras concepts</h1>\n",
    "<ul>\n",
    "    <li><b>Layers</b> are the building blocks.\n",
    "        <ul>\n",
    "            <li>To begin with, we will use <b>dense layers</b>.</li>\n",
    "            <li>The activation functions of <em>hidden layers</em> are open for you to choose, \n",
    "                e.g. sigmoid or ReLU.\n",
    "            </li>\n",
    "            <li>But the activation functions of <em>output layers</em> are determined by the task:\n",
    "                <ul>\n",
    "                    <li>Regression: linear activation function (default);</li>\n",
    "                    <li>Binary classification: sigmoid activation function; and</li>\n",
    "                    <li>Multiclass classification: softmax activation function.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Layers are combined into <b>networks</b>:\n",
    "        <ul>\n",
    "            <li>Consecutive layers must be compatible: the shape of the input to one layer is the shape of \n",
    "                the output of the preceding layer.\n",
    "            </li>\n",
    "            <li>In early lectures, we only consider a stack of layers but Keras allows directed acyclic graphs \n",
    "                and, later, we will briefly discuss some examples that are not just stacks of layers.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Once the network is built, we <b>compile</b> it, specifying:\n",
    "        <ul>\n",
    "            <li>A <b>loss function</b>:\n",
    "                <ul>\n",
    "                    <li>Regression, e.g. mean-squared-error (<code>mse</code>);</li>\n",
    "                    <li>Binary classification, e.g. (binary) cross-entropy (<code>binary_crossentropy</code>);</li>\n",
    "                    <li>Multiclass classification, e.g. (categorical) cross-entropy \n",
    "                        (<code>sparse_categorical_crossentropy</code> if the labels are encoded as\n",
    "                        integer labels\n",
    "                        or <code>categorical_crossentropy</code> if the integer labels have also been one-hot\n",
    "                        encoded).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>An <b>optimizer</b>, such as SGD &mdash; but see below.</li>\n",
    "            <li>A list of metrics to monitor during training and testing:\n",
    "                <ul>\n",
    "                    <li>Regression, e.g. mean-absolute-error (<code>mae</code>);</li>\n",
    "                    <li>Classification, e.g. accuracy (<code>acc</code>).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Keras optimizers</h1>\n",
    "<ul>\n",
    "    <li>We know about Gradient Descent: Batch, Mini-Batch, Stochastic.</li>\n",
    "    <li>Without going into details, many other variants of Gradient Descent have been devised (e.g. RMSprop, Adam, Adagrad, &hellip;):\n",
    "        <ul>\n",
    "            <li>some may have better convergence behaviour in the case of local minima;</li>\n",
    "            <li>some may converge more quickly.</li>\n",
    "        </ul>\n",
    "        although a disadvantage is that they typically introduce further hyperparameters\n",
    "        (e.g. momentum) in addition to learning rate.\n",
    "    </li>\n",
    "    <li>We'll use RMSprop below.\n",
    "        <ul>\n",
    "            <li>Be aware, its default learning rate is 0.001. This is usually OK, but in some cases you may need  \n",
    "                to change it.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Be aware too that there is an argument called <code>batch_size</code>. Assuming we set its value to                   somewhere between 1 and the size of the training set then we are getting Mini-Batch Gradient Descent.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using tensorflow as backed, you may get warnings that explain that tensorflow was \n",
    "# originally compiled on a different computer architecture from the one you are using. \n",
    "# This means its performance may not be optimal. The warning explains that, if you want \n",
    "# to optimize tensorflow for your architecture, then you need to rebuild (recompile)\n",
    "# tensorflow from scratch. We won't do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Regression</h1>\n",
    "<ul>\n",
    "    <li>For regression on structured/tabular data, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be the sigmoid function or ReLU.\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: just one output neuron (assuming we're predicting a single number per example).\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neuron should be the <b>linear function</b>: \n",
    "                        $g(z) = z$\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(There are also biases in each layer except the output layer &mdash; Keras will give us these \n",
    "        'for free'.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: Cork Property Prices</h2>\n",
    "<ul>\n",
    "    <li>We don't want too many hidden layers, nor too many neurons in each hidden layer. Why?</li>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with three inputs ($\\mathit{flarea}$, $\\mathit{bdrms}$, \n",
    "                $\\mathit{bthrms}$);\n",
    "            </li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function;</li>\n",
    "            <li>An output layer with a single neuron and linear activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We need to scale the features (Why?) But, since we are now not using scikit-learn's <code>ColumnTransformer</code>s to create a preprocessor, we need to take care of the scaling. Two approaches are\n",
    "        illustrated below.\n",
    "    </li>\n",
    "    <li>Our first approach is to write our own code to scale the data. Notice that the means and standard\n",
    "        deviations that are computed from the training set are used to standardize both the training set and\n",
    "        the test set, thus avoiding leakage.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_cork_X.mean(axis=0)\n",
    "st_devs = train_cork_X.std(axis=0)\n",
    "\n",
    "train_cork_X -= means\n",
    "train_cork_X /= st_devs\n",
    "\n",
    "test_cork_X -= means\n",
    "test_cork_X /= st_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 11:04:01.163647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(3,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"linear\")(x)\n",
    "cork_model = Model(inputs, outputs)\n",
    "\n",
    "cork_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb09ee8970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cork_model.fit(train_cork_X, train_cork_y, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 15071.4570 - mae: 87.7902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.79022979736328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_mae = cork_model.evaluate(test_cork_X, test_cork_y)\n",
    "test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The disdavantage is the need to remember to run the preprocessing code.</li> \n",
    "    <li>The second approach is to use a layer in the neural network: <code>NormalizationLayer</code>. This layer\n",
    "        is smart enough to avoid leakage. The advantage\n",
    "        is that the preprocessing is 'packaged' together with the rest of the network.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"linear\")(x)\n",
    "cork_model = Model(inputs, outputs)\n",
    "\n",
    "cork_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb1343a100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cork_model.fit(train_cork_X, train_cork_y, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 15594.3311 - mae: 89.9410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.94100952148438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_mae = cork_model.evaluate(test_cork_X, test_cork_y)\n",
    "test_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Feel free to edit the code, e.g. add or remove hidden layers, change the number of neurons in the\n",
    "        hidden layers, change ReLU to sigmoid, change from RMSprop to another optimizer, change the learning rate,\n",
    "        change the number of epochs,\n",
    "        or change the batch size.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Binary Classification</h1>\n",
    "<ul>\n",
    "    <li>For binary classification, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be sigmoid or ReLU.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: just one output neuron (for binary classification).\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neuron should be the sigmoid function also. Why?</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: CS1109 Dataset</h2>\n",
    "<ul>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with 3 inputs ($\\mathit{lect}$, $\\mathit{lab}$, $\\mathit{cao}$).</li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function.</li>\n",
    "            <li>An output layer with a single neuron and sigmoid activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We'll scale using a <code>Normalization</code> layer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(3,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "cs1109_model = Model(inputs, outputs)\n",
    "\n",
    "cs1109_model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb395a54f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs1109_model.fit(train_cs1109_X, train_cs1109_y, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 3.1151 - accuracy: 0.6522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6521739363670349"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = cs1109_model.evaluate(test_cs1109_X, test_cs1109_y)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Neural Network for Multi-Class Classification</h1>\n",
    "<ul>\n",
    "    <li>For multi-class classification, we might use a network with the following architecture:\n",
    "        <ul>\n",
    "            <li>Input layer: one input per feature.</li>\n",
    "            <li>Hidden layers: one or more hidden layers.\n",
    "                <ul>\n",
    "                    <li>Activation function for neurons in hidden layers can be sigmoid or ReLU.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Output layer: one output neuron per class.\n",
    "                <ul>\n",
    "                    <li>Activation function for the output neurons should be the softmax function.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What is softmax?  See lecture 15. Like sigmoid, it squashes outputs to $[0,1]$. But now there is one per class, so it also normalises them so that, together, they sum to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Example: Iris Dataset</h2>\n",
    "<ul>\n",
    "    <li>Let's start with this:\n",
    "        <ul>\n",
    "            <li>An input layer with 4 inputs (petal width and length, and sepal width and length).</li>\n",
    "            <li>Two hidden layers, with 64 neurons in each, and ReLU activation function.</li>\n",
    "            <li>An output layer with three neurons (one for Setosa, Versicolor and Virginica) and \n",
    "                softmax activation function.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(3, activation=\"softmax\")(x)\n",
    "iris_model = Model(inputs, outputs)\n",
    "\n",
    "iris_model.compile(optimizer=RMSprop(learning_rate=0.001), \n",
    "                   loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb134bac10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_model.fit(train_iris_X, train_iris_y, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1788 - accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333373069763"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = iris_model.evaluate(test_iris_X, test_iris_y)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Note the loss function above:\n",
    "        <ul>\n",
    "            <li><code>sparse_categorical_crossentropy</code> for multi-class classification when the classes\n",
    "                are integers, e.g. 0 = one kind of Iris, 1 = another kind, 2 = a third kind (which is what\n",
    "                we have in the Iris dataset).\n",
    "            </li>\n",
    "            <li><code>categorical_cross_entropy</code> for multi-class classification when the classes have\n",
    "                been one-hot encoded. See below.\n",
    "            </li>\n",
    "            <li>(And, as we've seen, <code>binary_crossentropy</code> for binary classification, where the classes\n",
    "                will be 0 or 1.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Below, an alternative, is code that illustrates one-hot encoding the target values using the Keras function called <code>to_categorical</code>, and then using <code>categorical_cross_entropy</code> for the loss function.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris_y = to_categorical(train_iris_y)\n",
    "test_iris_y = to_categorical(test_iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(4,))\n",
    "x = Normalization()(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(3, activation=\"softmax\")(x)\n",
    "iris_model = Model(inputs, outputs)\n",
    "\n",
    "iris_model.compile(optimizer=RMSprop(learning_rate=0.001), \n",
    "                   loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb395fe940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_model.fit(train_iris_X, train_iris_y, epochs=40, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fcb284310d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1661 - accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333373069763"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = iris_model.evaluate(test_iris_X, test_iris_y)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Neural networks are often not the best-peforming approaches for structured data. And, sure enough, our results here are not great. Of course, there is lots we can tweak to see if we can improve the results.</li>\n",
    "    <li>But, instead, let's switch to an image processing example.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Final Example: MNIST</h1>\n",
    "<ul>\n",
    "    <li>MNIST is a classic dataset for multi-class classification.</li>\n",
    "    <li>The task is classification of hand-written digits.\n",
    "        <ul>\n",
    "            <li>Features: 28 pixel by 28 pixel grayscale images of hand-written digits.\n",
    "                <ul>\n",
    "                    <li>The values are integers in $[0, 255]$.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Classes: 0 to 9.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Dataset: 70,000 images, so we can safely use holdout, and it is already partitioned:\n",
    "        <ul>\n",
    "            <li>60,000 training images;</li>\n",
    "            <li>10,000 test images.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras has a utility function for downloading it into four Numpy arrays\n",
    "# To get this to work on macOS, you might need to run something like this in a terminal:\n",
    "# $ /Applications/Python\\ 3.10/Install\\ Certificates.command\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 126 # Change this number to look at other images\n",
    "some_example = mnist_x_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  23,  79, 192, 216, 216,  91,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  13, 209, 252, 253, 252, 252, 227,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
       "        147, 209, 252, 252, 244, 168,  80,  31,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38, 175,\n",
       "        253, 252, 214, 139,  25,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 198, 253,\n",
       "        255, 209,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 114, 234, 252,\n",
       "        234,  28,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  23, 234, 252, 252,\n",
       "        100,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 252, 252, 151,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 154, 253, 253, 128,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 233,  22,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 252, 168,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 113, 253, 252, 168,   0,\n",
       "          0,  76, 113, 113,  51,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 114, 254, 253, 216, 191,\n",
       "        254, 253, 253, 253, 242, 141,  53,   4,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 253, 252, 252, 252,\n",
       "        253, 252, 252, 252, 253, 252, 252, 103,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252, 252, 214,\n",
       "        156,  56,  56, 106, 178, 252, 252, 228,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 153, 252, 252, 139,\n",
       "          0,   0,   0,   0,   4,  78, 252, 252, 101,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  76, 231, 253, 253,\n",
       "         76,   0,   0,   0,   4, 128, 253, 253, 114,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19, 190, 252,\n",
       "        244,  94,  57,  57, 179, 252, 252, 252,  88,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  19, 193,\n",
       "        253, 252, 252, 252, 253, 252, 252, 127,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  13,\n",
       "        153, 252, 252, 252, 253, 177,  52,   3,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the raw data for this image. Warning: large! (28 by 28)\n",
    "some_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw it\n",
    "some_example = some_example.reshape(28, 28)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(some_example, cmap=plt.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at its class\n",
    "mnist_y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We need to reshape:\n",
    "        <ul>\n",
    "            <li>Our training data is in a 3D array of shape (60000, 28, 28).</li>\n",
    "            <li>We change it to a 2D array of shape (60000, 28 * 28).\n",
    "                <ul>\n",
    "                    <li>This 'flattens' the images.</li>\n",
    "                    <li>When working with images, it is often better not to do this. In a future lecture, we'll\n",
    "                        build neural networks that do not require us to flatten.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Similarlly, the test data.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_x_train = mnist_x_train.reshape((60000, 28 * 28))\n",
    "\n",
    "mnist_x_test = mnist_x_test.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We'll do a two-layer network:\n",
    "        <ul>\n",
    "            <li>One hidden layer with 512 neurons, using the ReLU activation function.</li>\n",
    "            <li>The output layer will have 10 neurons, one per class, and \n",
    "                will use the softmax activation function.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The features (pixel values) are all in the same range $[0, 255]$, so we do not need to standardize\n",
    "        using a <code>Normalization</code> layer.\n",
    "    </li>\n",
    "    <li>But it is a bad idea to feed into a neural network values that are much larger than the initial weights, so\n",
    "        we will rescale to $[0, 1]$ by dividing by 255. We can do this using a <code>Rescaling</code> layer.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28 * 28,))\n",
    "x = Rescaling(scale=1./255)(inputs)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "mnist_model = Model(inputs, outputs)\n",
    "\n",
    "mnist_model.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 784)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Make sure you understand all the numbers above!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4315 - accuracy: 0.8917\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2168 - accuracy: 0.9384\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1695 - accuracy: 0.9516\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1399 - accuracy: 0.9599\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1198 - accuracy: 0.9662\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1045 - accuracy: 0.9706\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0932 - accuracy: 0.9736\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0837 - accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0763 - accuracy: 0.9784\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0698 - accuracy: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb1a181df0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model.fit(mnist_x_train, mnist_y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7fcb0a0b7af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0850 - accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9751999974250793"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = mnist_model.evaluate(mnist_x_test, mnist_y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Compare training accuracy and test accuracy.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In the 1960s, 70s, 80s and to some extent 90s, the typical pipeline for a computer vision (or image processing) system was as follows:\n",
    "        <ul>\n",
    "            <li>There would be a module that would extract features from the images. These features would have been carefully hand-designed. They might include edges detected by some edge detection algorithm, for example. (If you are interested, look up SIFT or SURF or HOG.)</li>\n",
    "            <li>Then these features would be fed into a typical learning algorithm, e.g. logistic regression.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Notice how different life is now &mdash; when using neural networks.\n",
    "        <ul>\n",
    "            <li>There's no extraction of hand-crafted features. We feed in the raw pixel values (or, lightly-processed pixel values, e.g. scaled values).</li>\n",
    "            <li>It is the layers of the neural network that automatically <em>discover</em> the features, and the final layer that makes the classification.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>As we will see, dense layers are only one possibility. Computer vision (image processing) more often also uses convolutional layers, pooling layers, batch normalization layers, and so on. We will study these soon.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Concluding Remarks</h1>\n",
    "<ul>\n",
    "    <li>A few decisions are constrained: number of inputs; number of output neurons; activation\n",
    "        function of output neurons; and (to some extent) loss function.\n",
    "    </li>\n",
    "    <li>But there are numerous hyperparameters (and even more to come!)\n",
    "        <ul>\n",
    "            <li>Even making a good guess at them is more art than science, although this is changing.</li>\n",
    "            <li>On the other hand, grid search or randomized search will make things even slower than they\n",
    "                already are &mdash; and we still have to specify some sensible values for\n",
    "                them to search through.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There is a considerable risk of overfitting.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
